---
layout: post
title: Representation Theorem and Neural Networks — Notes on Girosi and Poggio
---

Thoughts on [Girosi and Poggio](http://cbcl.mit.edu/people/poggio/journals/girosi-poggio-NeuralComputation-1989.pdf)

We can think of neural networks as approximations of differentiable multi variable functions. 

From the Representation Theorem (Kolmogorov and Arnol’d), a multi variable continuous function can be represented as a composition of continuous univariate functions.

These continuous functions can be stacked as a wide two layer network.

If such a two layer neural network can represent all multivariate continuous functions, why do we need deep architectures ?

Girosi and Poggio say the following in this regard:

>*1/ The inner univariate functions typically are not smooth. The lack of smoothness in these inner functions limit a two layer network architecture to generalize.*

I think this relates to the shallow/wide (vs) narrow/deep networks argument.

How are smoothness, continuity and generalizability related ?

Suppose if we restrict the inner functions to be polynomials, can we put a bound on the number of layers required to approximate a multi variable continuous function with only polynomials as building blocks ?

>*2/ The inner function is as complex as the function to be approximated and is not parametrized. *

My interpretation of this argument is that is we cannot put knobs on a generic function for the inner layers and tune it to approximate the function we want to learn. This makes it impractical to learn the univariate functions from data.

---
layout: post
comments: true
tags:
- ml
- nn
- notes
title: Representation Theorem and Neural Networks — Notes on Girosi and Poggio
---

Thoughts on [Girosi and Poggio](http://cbcl.mit.edu/people/poggio/journals/girosi-poggio-NeuralComputation-1989.pdf)

We can think of neural networks as approximations of differentiable multivariable functions. 

From the Representation Theorem (Kolmogorov and Arnol’d), a multivariable continuous function can be represented as a composition of continuous univariate functions.

$$
  \displaystyle f(x_1, x_2, .., x_n) = \sum_{q=1}^{2n+1}g[\sum_{p=1}^{n} l_ph_q(x_p)]
$$

These continuous functions can be stacked as a wide two layer network.

The first layer has $$2n+1$$ $$h_q$$ functions, the second layer has copies of a function $$g$$ applied on $$n$$ different sums.

If such a two layer neural network can represent all multivariate continuous functions, why do we need deep architectures ?

Girosi and Poggio say the following in this regard:

>*1/ The inner univariate functions $$h_q$$ typically are not smooth. The lack of smoothness in these inner functions limit a two layer network architecture to generalize.*

I think this relates to the shallow/wide (vs) narrow/deep networks argument.

Something to think about: How are smoothness, continuity and generalizability related ?

Also, suppose if we restrict the inner functions to be polynomials, can we put a bound on the number of layers required to approximate a multivariable continuous function with only polynomials as building blocks ?

>*2/ The inner function $$g$$ is as complex as the function to be approximated and is not parametrized. *

My interpretation of this argument is that is we cannot put knobs on a generic function $$g$$ and tune it to approximate the function $$f$$ we want to learn. This makes it impractical to learn the build block functions from data.

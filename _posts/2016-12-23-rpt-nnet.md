---
layout: post
title: Representation Theorem and Neural Networks — Notes on Girosi and Poggio
---

The Representation Theorem and Neural Networks — Notes on Girosi and Poggio

Thoughts on http://cbcl.mit.edu/people/poggio/journals/girosi-poggio-NeuralComputation-1989.pdf

We can think of neural networks as approximations of differentiable multi variable functions. 

From the Representation Theorem (Kolmogorov and Arnol’d), a multi variable continuous function (f) can be represented as a composition of continuous univariate functions (g) and (h)

If two layer neural networks can represent all multivariate continuous functions, why do we need deep architectures ?

Girosi and Poggio say the following in this regard:

1/ The inner univariate functions typically are not smooth. The lack of smoothness in these inner functions limit a two layer network architecture to generalize. 

I think this relates to the shallow/wide (vs) narrow/deep networks argument.
How are smoothness and continuity related ?
Also, suppose if we restrict the inner functions to be polynomials, can we put a bound on the number of levels required to approximate a multi variable continous function with only polynomials as building blocks ?

2/ The inner function (g) is as complex as the function to be approximated and is not parametrized. 
My interpretation of this argument is that is we cannot put knobs on a generic (g) and tune it to approximate the function we want to learn. This makes it impractical to learn (g) from data.